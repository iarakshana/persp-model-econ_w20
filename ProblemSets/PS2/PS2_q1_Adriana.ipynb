{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Integration\n",
    "### by [Richard W. Evans](https://sites.google.com/site/rickecon/), January 2020\n",
    "The code in this Jupyter notebook was written using Python 3.7. All references cited in this notebook are fully cited in Section 7 at the end of this notebook.\n",
    "1. Introduction\n",
    "2. Newton-Cotes Quadrature\n",
    "3. Gaussian Quadrature\n",
    "4. Monte Carlo Integration\n",
    "5. Sparse Grid Methods\n",
    "6. Discrete Markov Processes\n",
    "7. References\n",
    "\n",
    "This notebook currently has exercises for Sections 1 through 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "Integrals of the form $\\int_a^b g(x)dx$ arise often in economic models. One example is the aggregating of consumption amounts of a continuum of differentiated goods $c_t(i)$,\n",
    "\\begin{equation}\\label{NumInt_EqArmAggr}\n",
    "    C_t = \\left(\\int_0^1 \\alpha_i^\\frac{1}{\\varepsilon}c_t(i)^\\frac{\\varepsilon-1}{\\varepsilon}di\\right)^\\frac{\\varepsilon}{\\varepsilon-1}\n",
    "\\end{equation}\n",
    "where $C_t$ is aggregate consumption, $\\alpha_i$ is a weight on the particular amount of consumption of good $i$, $\\varepsilon$ is the constant elasticity of substitution between different goods $i$, and the measure of goods is normalized to be between 0 and 1, without loss of generality. This consumption aggregator is often called the Armington aggregator as it was first proposed in Armington (1969). It is also known as a Dixit-Stiglitz aggregator after its use in Dixit and Stiglitz (1977). Another key example of an integral that often occurs in macroeconomics is the expectations operator on the right-hand-side of the standard intertermporal Euler equation,\n",
    "\\begin{equation}\\label{NumInt_EqEulEx}\n",
    "  \\begin{split}\n",
    "    u'(c_t) &= \\beta E_{z_{t+1}|z_t}\\Bigl[(1+r_{t+1}-\\delta)u'(c_{t+1})\\Bigr] \\\\\n",
    "    \\Rightarrow\\quad u'(c_t) &= \\beta\\int_a^b \\Bigl(1+r_{t+1}(z_{t+1})-\\delta\\Bigr)u'\\Bigl(c_{t+1}(z_{t+1})\\Bigr)f(z_{t+1}|z_t)dz_{t+1}\n",
    "  \\end{split}\n",
    "\\end{equation}\n",
    "where $a$ and $b$ are the bounds of the support of $z_{t+1}$ and $f(z_{t+1}|z_t)$ is the pdf of $z_{t+1}$ that could potentially be conditional on $z_t$.\n",
    "\n",
    "It is a rare convenience when these integrals can be evaluated analytically. However, it does not take much richness in functional form to render analytical solutions impossible for many integrals in economic models. In these cases, the integral must be computed numerically. The following discussion of numerical integration draws from the great treatments of the subject in Heer and Maussner (2009, pp. 598-603), Judd (1998, ch. 7), and Adda and Cooper (2003, pp. 55-60)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Newton-Cotes Quadrature\n",
    "Newton-Cotes quadrature forumalas approximate the integral of a function $\\int_a^b g(x)dx$ by evaluating the function at $N$ equally spaced nodes $\\{x_1,x_2,...x_N\\}$ and weighting those nodes with $N$ weights $\\{\\omega_1,\\omega_2,...\\omega_N\\}$. The general form of Newton-Cotes quadrature forumulas is\n",
    "\\begin{equation}\\label{NumInt_EqNewtCotesGen}\n",
    "  \\int_a^b g(x)dx \\approx \\sum_{n=1}^N\\omega_n g(x_n)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Midpoint rule (1 node)\n",
    "The midpoint rule is the simplest Newton-Cotes formula and uses only one node or evaluation of the function. It is simply a Riemann sum approximation over the domain of the function $g(x)$. The midpoint formula simply evaluates the function at the midpoint of the domain of $x = \\frac{a+b}{2}$ and assumes that the function is a constant at that level over the entire domain of $x\\in[a,b]$.\n",
    "\\begin{equation}\\label{NumInt_EqMidPtRule}\n",
    "  \\int_a^b g(x)dx \\approx (b-a)g\\left(\\frac{a+b}{2}\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'images/Riemann_sum_convergence.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-eaff21dd0890>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m        'master/NumIntegr/images/Riemann_sum_convergence.png')\n\u001b[0;32m      7\u001b[0m \u001b[0mimage_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'images/Riemann_sum_convergence.png'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'images/Riemann_sum_convergence.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'images/Riemann_sum_convergence.png'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from IPython.display import Image\n",
    "\n",
    "# Download and save the data file Riemann_sum_convergence.png\n",
    "url = ('https://raw.githubusercontent.com/rickecon/Notebooks/' +\n",
    "       'master/NumIntegr/images/Riemann_sum_convergence.png')\n",
    "image_file = requests.get(url, allow_redirects=True)\n",
    "open('images/Riemann_sum_convergence.png', 'wb').write(image_file.content)\n",
    "Image('images/Riemann_sum_convergence.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more sophisticated midpoint rule is the composite midpoint rule, which breaks up the domain of the function $g(x)$ into $N$ intervals and applies the midpoint rule to each interval. For nodes $x_0,x_1,\\ldots,x_{N-1}$ with $x_i=a+\\frac{(2i+1)(b-a)}{2N}$, the composite midpoint rule is given by\n",
    "\\begin{equation}\\label{NumInt_EqMidPtRuleComp}\n",
    "  \\int_a^b g(x)dx \\approx \\frac{b-a}{N}\\sum_{i=0}^{N-1}g(x_i)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download and save the data file MidRiemann2.png\n",
    "url = ('https://raw.githubusercontent.com/rickecon/Notebooks/' +\n",
    "       'master/NumIntegr/images/MidRiemann2.png')\n",
    "image_file = requests.get(url, allow_redirects=True)\n",
    "open('images/MidRiemann2.png', 'wb').write(image_file.content)\n",
    "Image('images/MidRiemann2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Trapezoid rule (2 nodes)\n",
    "The trapezoid rule estimates the integral as the area under a line that connects the function $g(x)$ at the two endpoints $a$ and $b$.\n",
    "\\begin{equation}\\label{NumInt_EqTrapRule}\n",
    "  \\int_a^b g(x)dx \\approx \\frac{b-a}{2}\\bigl[g(a) + g(b)\\bigr]\n",
    "\\end{equation}\n",
    "\n",
    "A more sophisticated trapezoid rule is the composite trapezoid rule, which breaks up the domain of the function $g(x)$ into $N$ intervals and applies the trapezoid rule to each interval.  For nodes $x_0,x_1,\\ldots,x_N$ with $x_i=a+i(b-a)/N$, the composite trapezoid rule is given by\n",
    "\\begin{equation}\\label{NumInt_EqTrapRuleComp}\n",
    "  \\int_a^b g(x)dx \\approx \\frac{b-a}{2N}\\left[g(x_0) + 2\\sum_{i=1}^{N-1}g(x_i) + g(x_N)\\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and save the data file Integration_num_trapezes_notation.png\n",
    "url = ('https://raw.githubusercontent.com/rickecon/Notebooks/' +\n",
    "       'master/NumIntegr/images/Integration_num_trapezes_notation.png')\n",
    "image_file = requests.get(url, allow_redirects=True)\n",
    "open('images/Integration_num_trapezes_notation.png', 'wb').write(image_file.content)\n",
    "Image('images/Integration_num_trapezes_notation.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Simpson's rule (3 nodes)\n",
    "Simpson's rule offers a smooth nonlinear (quadratic) alternative the linear approximations of the midpoint and trapezoid rules. Simpson's rule finds the unique quadratic function in $x$ that passes through the end points and the midpoint of the function $g(a)$, $g\\left(\\frac{a+b}{2}\\right)$, and $g(b)$, which produces the following weights and values.\n",
    "\\begin{equation}\\label{NumInt_EqSimpsRule}\n",
    "  \\int_a^b g(x)dx \\approx \\frac{b-a}{6}\\left[g(a) + 4g\\left(\\frac{a+b}{2}\\right) + g(b)\\right]\n",
    "\\end{equation}\n",
    "\n",
    "Again, a more sophisticated Simpson's rule is the composite Simpson's rule, which breaks up the domain of the function $g(x)$ into $2N$ intervals and applies the Simpson's rule to each interval.  For nodes $x_0,x_1,\\ldots,x_{2N}$ with $x_i=a+i(b-a)/(2N)$, the composite Simpson's rule is given by\n",
    "\\begin{equation}\\label{NumInt_EqTrapRuleComp2}\n",
    "  \\int_a^b g(x)dx \\approx \\frac{b-a}{6N}\\left[g(x_0) + 4\\sum_{i=1,3,\\ldots}^{2N-1}g(x_i) + 2\\sum_{i=2,4,\\ldots}^{2N-2}g(x_i) + g(x_{2N})\\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and save the data file Simpsons_method_illustration.png\n",
    "url = ('https://raw.githubusercontent.com/rickecon/Notebooks/' +\n",
    "       'master/NumIntegr/images/Simpsons_method_illustration.png')\n",
    "image_file = requests.get(url, allow_redirects=True)\n",
    "open('images/Simpsons_method_illustration.png', 'wb').write(image_file.content)\n",
    "Image('images/Simpsons_method_illustration.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.1.** You can verify that the analytical solution to the integral of the function\n",
    "\\begin{equation*}\n",
    "  g(x)=0.1x^4 -1.5x^3 + 0.53x^2 + 2x + 1\n",
    "\\end{equation*}\n",
    "between $x=-10$ and $x=10$ is $\\int_{-10}^{10} g(x)dx = 4,373.3\\bar{3}$. Write a Python function that will take as arguments an anonymous function that the user specifies representing $g(x)$, integration bounds $a$ and $b$, the number of intervals $N$, and\n",
    "```python\n",
    "method = {'midpoint', 'trapezoid', 'Simpsons'}\n",
    "```\n",
    "Using the composite methods, evaluate the numerical approximations of the integral $\\int_a^b g(x)dx$ using all three Newton-Cotes methods in your function and compare the difference between the values of these integrals to the true analytical value of the integral.\n",
    "\n",
    "**a)** Plot the absolute error of the difference between the approximated integral and the analytic integral for each of the three methods for different values of $N$ (number of bars). That is, create three different line plots--one for each Newton-Cotes rule--for which the $x$-axis is different numbers of nodes and the $y$-axis is the absolute approximation error for each number of nodes. Plot each of these lines for $N=[20, 21, 22, ... 200]$. You can create this vector with the code `Nvec = np.arange(20, 201, 1)`.\n",
    "\n",
    "**b)** Why are the methods ranked the way they are? [Hint: It has to do with the original function g(x).] You might want to plot the function $g(x)$ between -10 and 10 to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_mid(a,b,N):\n",
    "    midpoint = []\n",
    "    nodes = np.linspace(a,b,N+1)\n",
    "    list1 = [xi for xi in range(1, N+1)]\n",
    "    for i in list1:\n",
    "        midpoint.append((nodes[i-1]+nodes[i])*0.5)\n",
    "        \n",
    "    return midpoint\n",
    "\n",
    "def g_x(x):\n",
    "    g = 0.1*(x**4) - 1.5*(x**3) + 0.53*(x**2) + 2*x + 1\n",
    "    return g\n",
    "\n",
    "\n",
    "def get_integration(g_x,a,b,N,method = ('midpoint','trapezoid','Simpsons')):\n",
    "    \n",
    "    if method == 'midpoint':\n",
    "        nodes = np.linspace(a,b,N + 1)\n",
    "        midpoints = get_mid(a,b,N)\n",
    "        heights = []\n",
    "        list1 = [xi for xi in range(0, N)]\n",
    "        for i in list1:\n",
    "            x_mid = midpoints[i]\n",
    "            heights.append(g_x(x_mid))\n",
    "        sum_height = sum(a for a in heights)\n",
    "        bin_width = (b - a)/N\n",
    "    elif method =='trapezoid':\n",
    "        nodes = np.linspace(a,b,N + 1)\n",
    "        sum_height = g_x(nodes[0]) + g_x(nodes[-1]) + 2 * sum(g_x(nodes[1:-1]))\n",
    "        bin_width = (b - a)/(2 * N)\n",
    "    elif method =='Simpsons':\n",
    "        nodes = np.linspace(a,b,2 * N + 1)\n",
    "        bin_width = (b - a)/(6 * N)\n",
    "        sum_height = g_x(nodes[0]) + 4 * sum(g_x(nodes[1:-1][::2])) + 2 * sum(g_x(nodes[1:-1][1::2])) + g_x(nodes[-1])\n",
    "    else:\n",
    "        return 'no this type'\n",
    "    integration = bin_width * sum_height \n",
    "    \n",
    "    return integration\n",
    "\n",
    "mid_error = abs(get_integration(g_x,-10,10,200,'midpoint')-4373-1/3)\n",
    "tra_error = abs(get_integration(g_x,-10,10,200,'trapezoid')-4373-1/3)\n",
    "sim_error = abs(get_integration(g_x,-10,10,200,'Simpsons')-4373-1/3)\n",
    "\n",
    "print(\"midpoint's approximated value:\", get_integration(g_x,-10,10,200,'midpoint'))\n",
    "print(\"midpoint's absolute error:\", mid_error)\n",
    "print(\"trapezoid's approximated value:\", get_integration(g_x,-10,10,200,'trapezoid'))\n",
    "print(\"trapezoid's absolute error:\", tra_error)\n",
    "print(\"simpson's approximated value:\", get_integration(g_x,-10,10,200,'Simpsons'))\n",
    "print(\"simpson's absolute error:\", sim_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part a\n",
    "Nvec = np.arange(20, 201, 1)\n",
    "\n",
    "nvec_mid = np.arange(20, 201, 1, dtype='float')\n",
    "nvec_tra = np.arange(20, 201, 1, dtype='float')\n",
    "nvec_sim = np.arange(20, 201, 1, dtype='float')\n",
    "\n",
    "for i in range(20, 201):\n",
    "    nvec_mid[i - 20] = abs(get_integration(g_x,-10,10,i,'midpoint')-4373-1/3)\n",
    "    nvec_tra[i - 20] = abs(get_integration(g_x,-10,10,i,'trapezoid')-4373-1/3)\n",
    "    nvec_sim[i - 20] = abs(get_integration(g_x,-10,10,i,'Simpsons')-4373-1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part a plot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "ax = plt.gca()\n",
    "ax.plot(Nvec, nvec_mid, label='Midpoint')\n",
    "ax.plot(Nvec, nvec_tra, label='Trapezoid')\n",
    "ax.plot(Nvec, nvec_sim, label='Simpsons')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot g(x)\n",
    "xrange = np.linspace(-10, 10, 200)\n",
    "ax = plt.gca()\n",
    "ax.plot(xrange, g_x(xrange))\n",
    "ax.spines[\"bottom\"].set_position(\"zero\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simpsons appears to converge to 0 fastest, followed by the Midpoint Method and then Trapezoid Rule. As suggested by the hint, the key is in the g(x) that is a polynomial of order 4 so nonlinear in x. Trapezoid rule calculates areas based on the trapezoids so the change from convex to concave makes the convergence harder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.2.** Write a Python function that makes a Newton-Cotes discrete approximation of the distribution of the normally distributed variable $Z \\sim N(\\mu,\\sigma)$. Let this function take as arguments the mean $\\mu$, the standard deviation $\\sigma$, the number of equally spaced nodes $N$ to estimate the distribution, and the number of standard deviations $k$ away from $\\mu$ to make the furthest nodes on either side of $\\mu$. Use the [`scipy.stats.norm.cdf`](http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html#scipy.stats.norm) command for the cdf of the normal distribution to compute the weights $\\omega_n$ for the nodes $x_n$. Have this function return a vector of nodes of $[Z_1,Z_2,...Z_N]$ and a vector of weights $[\\omega_1,\\omega_2,...\\omega_N]$ such that $\\omega_i$ is given by the integral under the normal distribution between the midpoints of the two closest nodes. Define $f(Z;\\mu,\\sigma)$ as the pdf of the normal distribution and $F(Z;\\mu,\\sigma)$ as the cdf.\n",
    "\\begin{equation*}\n",
    "  \\begin{split}\n",
    "    &\\omega_i =\n",
    "      \\begin{cases}\n",
    "        F\\left(\\frac{Z_1 + Z_2}{2};\\mu,\\sigma\\right) \\quad\\quad\\quad\\quad\\:\\text{if}\\quad i = 1 \\\\\n",
    "        \\int_{Z_{min}}^{Z_{max}}f(Z;\\mu,\\sigma)dZ \\quad\\quad\\:\\:\\text{if}\\quad 1<i<N \\\\\n",
    "        1 - F\\left(\\frac{Z_{N-1} + Z_{N}}{2};\\mu,\\sigma\\right) \\quad\\text{if}\\quad i = N\n",
    "      \\end{cases} \\\\\n",
    "    &\\text{where}\\quad Z_{min} = \\frac{Z_{i-1} + Z_{i}}{2} \\quad\\text{and}\\quad Z_{max} = \\frac{Z_{i} + Z_{i+1}}{2}\n",
    "  \\end{split}\n",
    "\\end{equation*}\n",
    "What are the weights and nodes $\\{\\omega_n,Z_n\\}_{n=1}^N$ for $\\mu=5$, $\\sigma=1.5$,$N=11$, and $k=3$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_z(m,k,s,N):\n",
    "    array_z = np.linspace(m-k*s,m+k*s,N)\n",
    "    list_z = array_z.tolist()\n",
    "    return list_z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z(i,m,k,s,N):\n",
    "    z_list = get_z(m,k,s,N)\n",
    "    z_value = z_list[i-1]\n",
    "    return z_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import pandas as pd\n",
    "def get_weight(m,k,s,N):\n",
    "    w = []\n",
    "    list = [xi for xi in range(1, N+1)]\n",
    "    for i in list:\n",
    "        if i == 1:\n",
    "            weight = norm.cdf((z(1,m,k,s,N)+z(2,m,k,s,N))/2,m,s)\n",
    "            w.append(weight)\n",
    "        elif i == N:\n",
    "            weight = 1- norm.cdf((z(N-1,m,k,s,N)+z(N,m,k,s,N))/2,m,s)\n",
    "            w.append(weight)\n",
    "        else:\n",
    "            zmin = (z(i-1,m,k,s,N)+z(i,m,k,s,N))/2\n",
    "            zmax = (z(i,m,k,s,N)+z(i+1,m,k,s,N))/2\n",
    "            weight = norm.cdf(zmax,m,s)-norm.cdf(zmin,m,s)\n",
    "            w.append(weight)\n",
    "  \n",
    "    return w\n",
    "\n",
    "def get_node_weight(m,k,s,N):\n",
    "    weight_list = get_weight(m,k,s,N)\n",
    "    node_list = get_z(m,k,s,N)\n",
    "    dic = {'nodes':node_list,'weights':weight_list}\n",
    "    df = pd.DataFrame(dic)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_node_weight(0,3,1,11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.3.** If $Z\\sim N(\\mu,\\sigma)$, then $A\\equiv e^Z\\sim LN(\\mu,\\sigma)$ is distributed lognormally and $\\log(A)\\sim N(\\mu,\\sigma)$. Use your knowledge that $A\\equiv e^Z$, $\\log(A)\\sim N(\\mu,\\sigma)$, and your function from Exercise 2.2 to write a function that gives a discrete approximation to the lognormal distribution. Note: You will not end up with evenly spaced nodes $[A_1,A_2,...A_N]$, but your weights should be the same as in Exercise 2.2. What are the resulting weights and nodes $\\{\\omega_n,A_n\\}_{n=1}^N$ for the calibration from Exercise 2.2: $\\mu=5$, $\\sigma=1.5$, $N=11$, and $k=3$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_node_weight2(m,k,s,N):\n",
    "    a_value = []\n",
    "    wei = get_weight(m,k,s,N)\n",
    "    loga_list = get_z(m,k,s,N)\n",
    "    \n",
    "    for a in loga_list:\n",
    "        a_value.append((math.e)**a)\n",
    "    d = {'nodes':a_value,'weights':wei}\n",
    "    df_nw = pd.DataFrame(d)\n",
    "    \n",
    "    \n",
    "    return df_nw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_node_weight2(0,3,1,11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.4.** Let $Y_i$ represent the income of individual $i$ in the United States for all individuals $i$. Assume that income $Y_i$ is lognormally distributed in the U.S. according to $Y_i\\sim LN(\\mu,\\sigma)$, where the mean of log income is $\\mu = 10.5$ and the standard deviation of log income is $\\sigma = 0.8$. Use your function from Exercise 2.3 to compute a discrete approximation of the expected value of income or average income in the U.S using $N = 11$ and $k=3$. How does your approximation compare to the exact expected value of $E[Y] = e^{\\mu + \\frac{\\sigma^2}{2}}$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to calculate the exact expected value\n",
    "m = 10.5 \n",
    "s = 0.8\n",
    "exact_expected = (math.e)**(m+0.5*(s**2))\n",
    "\n",
    "df_appr =  get_node_weight2(10.5,3,0.8,19)\n",
    "appr_avg = sum(df_appr.nodes*df_appr.weights)\n",
    "abs_error = abs(appr_avg-exact_expected)\n",
    "print(\"The approximated expected value is\",appr_avg)\n",
    "print(\"The absolute error is\",abs_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gaussian Quadrature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Gaussian Quadrature\n",
    "In Newton-Cotes quadrature, the nodes are uniformly spaced. Gaussian quadrature formulas for approximating an integral take the same approximation form $\\int_a^b g(x)dx \\approx \\sum_{n=1}^N\\omega_n g(x_n)$ and optimally choose the weights $\\omega_n$ and unevenly spaced nodes $x_n$ given the total number of nodes $N$ and some approximating polynomial class $h_i(x)$. The $N$ weights and nodes are chosen to make an *exact integration* relationship hold. That is, for polynomials of order $2N-1$ or less, the $N$ weights and nodes must exactly satisfy\n",
    "\\begin{equation}\\label{NumInt_EqGausQuadGen}\n",
    "  \\int_a^b g_(x)dx \\approx \\int_a^b h_i(x)dx = \\sum_{n=1}^N \\omega_n h_i(x_n) \\quad\\text{for}\\quad i=0,1,\\ldots,2N-1\n",
    "\\end{equation}\n",
    "where $h_i(x)$ is an $i$-order polynomial in $x$.  If the $h_i(x)$ form a basis, this means that every polynomial of degree less than or equal to $2N-1$ will be computed exactly using the $N$ weights and $N$ nodes.\n",
    "\n",
    "As a simple example, suppose we want to approximate an arbitrary function $g(x)$ with Gaussian quadrature using a simple class of polynomials $h_i(x) = x^{i}$ and only $N=2$ weights and nodes. The Gaussian quadrature definition equation above implies a system of four equations used to determine the four variables $(\\omega_1,\\omega_2,x_1,x_2)$ to approximate the integral $\\int_a^b g(x)dx \\approx \\sum_{n=1}^N\\omega_n g(x_n)$.\n",
    "\\begin{equation}\\label{NumInt_EqGausQuadN2}\n",
    "  \\begin{split}\n",
    "    &\\int_a^b dx = \\omega_1 + \\omega_2 \\\\\n",
    "    &\\int_a^b x dx = \\omega_1 x_1 + \\omega_2 x_2 \\\\\n",
    "    &\\int_a^b x^2 dx = \\omega_1 x_1^2 + \\omega_2 x_2^2 \\\\\n",
    "    &\\int_a^b x^3 dx = \\omega_1 x_1^3 + \\omega_2 x_2^3\n",
    "  \\end{split}\n",
    "\\end{equation}\n",
    "For $N=2$, the optimal weights and nodes that solve the system above are $(\\omega_1,\\omega_2,x_1,x_2)=(1,1,-0.578,0.578)$. The Python code to solve this nonlinear system could be a simple root finder such as [`scipy.optimize.root`](http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.root.html#scipy.optimize.root) or one of the constrained minimizers in [`scipy.optimize.minimize`](http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize). In general, the spacing of the nodes will not be uniform. These nodes and weights will exactly solve the integral of a polynomial of order 3 ($2N-1$).\n",
    "\n",
    "Some Gaussian quadrature principles:\n",
    "* The nodes $x_n$ and weights $\\omega_n$ for Gaussian quadrature are fixed for a given $N$ and basis function $h_i(x)$\n",
    "* Gaussian quadrature with $N$ nodes is exact for polynomial functions of order up to $2N-1$.\n",
    "* For a general function, gaussian quadrature approximation error is in proportion to the degree that the function is approximated by a polynomial function.\n",
    "* The accuracy of the Gaussian quadrature approximation of the integral $\\int_a^b g(x)dx$ increases in the number of nodes $N$.\n",
    "* The accuracy of the approximation of the integral can also be improved by the choice of polynomial family $h_i(x)$. In particular, the families of orthonormal polynomials have multiple desirable properties. Because of the orthogonality of their coefficients, the system above is easier to solve due to the lack of collinearity. Also, the weights $\\omega_n$ turn out to be the zeros of the orthogonal polynomial family. Lastly, these orthogonal families of polynomials can give very accurate solutions to integrals of the form $\\int_a^b w(x)g(x)dx$, where $w(x)$ is the weighting function of an orthonormal family of polynomials.\n",
    "\n",
    "For finite integration limits, [`scipy.integrate.quad`](http://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.quad.html#scipy.integrate.quad) uses a Clenshaw-Curtis method which uses Chebyshev orthogonal polynomials as basis functions and uses corresponding Chebyshev moments. If one of the integration limits is infinite, [`scipy.integrate.quad`](http://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.quad.html#scipy.integrate.quad) uses Fourier basis functions with the corresponding Fourier moments.\n",
    "\n",
    "For a more detailed discussion of the theory behind Gaussian quadrature, see Judd (1998, pp. 257-265) and Heer and Maussner (2009, pp. 599-601). The general applicability of Gaussian quadrature and its accuracy and efficiency advantage over Newton-Cotes formulas is summarized by Judd (1998, p. 265).\n",
    "> \"Even when the asymptotic rate of convergence for Gaussian quadrature is no better than the comparable Newton-Cotes formula, experience shows that Gaussian formulas often outperform the alternative Newton-Cotes formula [in terms of accuracy].\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.1.** Approximate the integral of the function in Exercise 2.1 using Gaussian quadrature with $N=3$, $(\\omega_1,\\omega_2,\\omega_3,x_1,x_2,x_3)$. Solve for the Gaussian quadrature nodes and weights using the class of polynomials $h_i(x)=x^i$. How does the accuracy of your approximated integral compare to the approximations from Exercise 2.1 using $N=3$ for the midpoint rule, trapezoid rule, and Simpson's rule, and the true known analytical value of the integral?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "def equation(x):\n",
    "    \n",
    "    return [x[0] + x[1] + x[2] - 2,\n",
    "           x[0] * x[3] + x[1] * x[4] + x[2] * x[5] - 0,\n",
    "           x[0] * x[3]**2 + x[1] * x[4]**2 + x[2] * x[5]**2 - 2/3,\n",
    "           x[0] * x[3]**3 + x[1] * x[4]**3 + x[2] * x[5]**3 - 0,\n",
    "           x[0] * x[3]**4 + x[1] * x[4]**4 + x[2] * x[5]**4 - 2/5,\n",
    "           x[0] * x[3]**5 + x[1] * x[4]**5 + x[2] * x[5]**5 - 0]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = optimize.root(equation, [0, 0, 0, 0, 0, 0])\n",
    "weight_x = output.x\n",
    "a = -10\n",
    "b = 10\n",
    "c = (b - a) / 2\n",
    "d = (b + a) / 2\n",
    "gau_appr = c * (weight_x[0] * g_x(c * weight_x[3] + d) + \n",
    "                       weight_x[1] * g_x(c * weight_x[4] + d) + \n",
    "                       weight_x[2] * g_x(c * weight_x[5] + d))\n",
    "gau_error = abs(gau_appr - 4373 - 1/3)\n",
    "\n",
    "print('Gaussian approximation:', gau_appr)\n",
    "print('Absolute error for Gaussian approximation:', gau_error)\n",
    "print('Gaussian has smaller absolute error than Newton-Cotes')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.2.** Use the Python Gaussian quadrature command [`scipy.integrate.quad`](http://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.quad.html#scipy.integrate.quad) to numerically approximate the integral from Exercise 2.1.\n",
    "\\begin{equation*}\n",
    "  \\int_{-10}^{10} g(x)dx \\quad\\text{where}\\quad g(x)=0.1x^4 -1.5x^3 + 0.53x^2 + 2x + 1\n",
    "\\end{equation*}\n",
    "How does the approximated integral using the [`scipy.integrate.quad`](http://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.quad.html#scipy.integrate.quad) command compare to the known analytical value of the integral?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import integrate\n",
    "quad_approximation = integrate.quad(g_x, -10, 10)\n",
    "quad_error = abs(quad_approximation[0] - 4373 - 1/3)\n",
    "print('Quad-Approximation:', quad_approximation[0])\n",
    "print('The absolute error for this method is', quad_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Monte Carlo Integration\n",
    "High-dimensional integration is highly inefficient using the standard one-dimensional methods of Newton-Cotes and Gaussian quadrature.  The method of choice in high-dimensional settings is known as Monte Carlo Integration.\n",
    "\n",
    "In this section, we detail two types of Monte Carlo integration. The standard Monte Carlo simulation approach in Section 4.1 uses pseudo-randomly generated draws from a uniform distribution over the domain of the function in order to approximate the weights and nodes for integration. The quasi-Monte Carlo approach in Section 4.2 uses elements of low-discrepancy sequences over the domain of the function in order to approximate the integral. Both methods have benefits and drawbacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Standard Monte Carlo integration\n",
    "In the Newton-Cotes quadrature methods of approximating an integral, nodes and weights for the approximation $\\sum_{n=1}^N\\omega_n g(x_n)$ are chosen without much attention to the effect of the placement of these nodes or the levels of the weights on the accuracy of the approximation. Newton-Cotes methods are computationally fast, but lack in accuracy. Gaussian quadrature methods spend more computational time choosing \"optimal\" weights and nodes, but this gives an accuracy payoff over Newton-Cotes formulas. Monte Carlo integration methods use the computationally fast method of drawing uniformly from the support of the variable of integration. These methods depend on a large number of draws to get high accuracy. Judd (1998, pp. 309-311) spends significant time explaining that these methods are more correctly called \"pseudo-Monte Carlo methods\" because they make use of pseudorandom number generators, the use of which cannot invoke the law of large numbers or the central limit theorem. However, the biases introduced by pseudorandom number generators are rarely significant in practice. Although Monte Carlo integration methods do not converge as quickly as Gaussian quadrature methods for functions of one variable, they are especially valuable when integrating over functions of multiple variables.\n",
    "\n",
    "Let $\\Omega\\subset\\mathbb{R}^m$ be the domain of integration. Let $\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_N$ be $N$ uniform random draws from $\\Omega$. Then we can write the following approximation of the integral.\n",
    "\\begin{equation}\\label{EqMontoCarloIntGen}\n",
    "  \\int_\\Omega g(\\mathbf{x})d\\mathbf{x}\\approx V\\frac{1}{N}\\sum_{n=1}^N g\\left(\\mathbf{x}_n\\right) \\quad\\text{where}\\quad V = \\int_\\Omega d\\mathbf{x}\n",
    "\\end{equation}\n",
    "The equation above says we can approximate the integral of a function $g(\\mathbf{x})$ on a domain $\\Omega$ by taking the average of the evaluations of the function $g$ at $N$ random draws of the vector $\\mathbf{x}_n$ multiplied by the volume of the domain.\n",
    "\n",
    "An easy example of a univariate integral is $\\int_0^1 x\\: dx$ (here $g(\\mathbf{x}) = x$). The Monte Carlo approximation formula for this integral is the following.\n",
    "\\begin{equation}\\label{EqMontCarlIntXdX}\n",
    "  \\int_0^1 x\\: dx \\approx V\\frac{1}{N}\\sum_{n=1}^N x_n = \\frac{1}{N}\\sum_{n=1}^N x_n\n",
    "\\end{equation}\n",
    "It is easy to see that the answer to the exact integral on the left-hand-side of this integral is 1/2. In the approximation on the right-hand-side of this equation, $V$ is the volume of the domain of $x\\in[0,1]$, which is 1. It is straightforward to see that the average of $N$ draws from a uniform distribution between 0 and 1 will converge quickly to 1/2.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integrate f(x) = x between 0 and 2\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as sts\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# # Approximate integral int_0^1 x dx by Monte Carlo integration\n",
    "np.random.seed(seed=25)\n",
    "\n",
    "N = 10\n",
    "mc_draws = 2 * sts.uniform.rvs(size=N)\n",
    "# print(mc_draws)\n",
    "approx_int = 2 * (1 / N) * mc_draws.sum()\n",
    "print('For N=', N, ', MC approx integral =', approx_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 4.1 lets you try your hand at coding a classic Monte Carlo integration approximation of the integral of a function of two variables to approximate the value of $\\pi$. The area of a circle with radius $r=1$ is $\\pi$ ($\\pi r^2$). A way to visualize the Monte Carlo approximation of the area of that circle, or $\\pi$, is to enclose the circle in a square with sides of length 2, in which the $x$-axis goes from -1 to 1 and the $y$-axis goes from -1 to 1. The points in the Figure are the uniformly distributed random draws from $(x,y)\\in[-1,1]\\times[-1,1]$. Intuitively, the area of the circle is the fraction of the dots (red dots divided by total dots) that are inside the circle or on the boundary of the circle, multiplied by the area or volume of the square in which the circle lies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and save the data file MonteCarloCircle.png\n",
    "url = ('https://raw.githubusercontent.com/rickecon/Notebooks/' +\n",
    "       'master/NumIntegr/images/MonteCarloCircle.png')\n",
    "image_file = requests.get(url, allow_redirects=True)\n",
    "open('images/MonteCarloCircle.png', 'wb').write(image_file.content)\n",
    "Image('images/MonteCarloCircle.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the intuition of the previous paragraph and of the figure above, the exact area of the circle can be written as an integral of the indicator function of coordinate variables $x$ and $y$ in the following way.\n",
    "\\begin{equation}\\label{EqMontCarlIntPiCircInd}\n",
    "  \\begin{split}\n",
    "    \\text{Area} &= \\int_\\Omega g(x,y)dx\\,dy = \\pi \\\\\n",
    "    &\\quad\\text{where}\\quad g(x,y) =\n",
    "      \\begin{cases}\n",
    "        1\\quad\\text{if}\\quad x^2 + y^2 \\leq 1 \\\\\n",
    "        0\\quad\\text{else}\n",
    "      \\end{cases} \\quad\\text{and}\\quad \\Omega = [-1,1]\\times[-1,1]\n",
    "  \\end{split}\n",
    "\\end{equation}\n",
    "The exact integral for the area of a unit radius circle can be Monte Carlo approximated using the form above resulting in the following function.\n",
    "\\begin{equation}\\label{EqMontCarlIntPiCirc}\n",
    "  \\int_\\Omega g(x,y)dx\\,dy \\approx 4\\frac{1}{N}\\sum_{n=1}^N g\\left(x_n,y_n\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Quasi-Monte Carlo integration\n",
    "It is important to realize what Monte Carlo methods really are in practice. Due to the impracticality of generating truly \"random\" sequences, Monte Carlo methods utilize pseudorandom sequences. Any sequence generated using a pseudorandom number generator will have a small amount of artificial correlation, and this problem is compounded in higher dimensions.\n",
    "\n",
    "Quasi-Monte Carlo methods dispense with the attempt to create deterministic samples that mimic random samples, and instead embrace their deterministic character. Judd (1998, ch. 9) defines quasi-Monte Carlo methods as sampling methods that do not rely on probabilistic ideas and pseudorandom sequences for constructing the sample and analyzing the estimate. Quasi-Monte Carlo methods use the same approximating function $\\sum_{n=1}^N\\omega_n g(x_n)$, but draw on number theory and Fourier analytic methods to create low-discrepancy sequences that are used as sample points. However, other than the selcetion of sample points, quasi-Monte Carlo integration proceeds in exactly the same way as standard Monte Carlo integration as detailed in the previous section.\n",
    "\n",
    "Many different deterministic sequences can be used in quasi-Monte Carlo sampling.  All of these \"quasirandom\" sequences strive for uniformity in a general sense. Hence it is useful to have a precise way of measuring the degree to which a point set exhibits uniformity. If we have a uniformly distributed sequence $\\mathbf{x}_n$ in the $s$-dimensional unit cube $I^s=[0,1)^s$, we would intuitively expect that every subset of $I^s$ with the same volume would contain the same number of points.  This idea is described precisely by the discrepancy of $\\mathbf{x}_n$. We first define local discrepancy, and then define global discrepancy.\n",
    "\n",
    "For $N$ points $\\{\\mathbf{x}_n\\}_{n=1}^N$ in $I^s$, $s\\geq0$, and $J\\subseteq I^s$, the local discrepancy $D(J;N)$ is defined by\n",
    "\\begin{equation}\\label{EqQMCLocalDiscrep}\n",
    "  D(J;N)=S(J;N)-V(J)N,\n",
    "\\end{equation}\n",
    "where $V(J)$ is the volume of the subinterval $J$ and $S(J;N)$ is the number of points from $\\{\\mathbf{x}_n\\}_{n=1}^N$ that are in $J$.  If the $N$ points are uniformly distributed, then the local discrepancy should be very small for all $J$'s.\n",
    "\n",
    "The figure below shows a $2$-dimensional unit cube $I^s=[0,1)^2$ with six points.  Three sub-intervals $A$, $B$, and $C$ are shaded. The local discrepancy of $A$, which contains $3$ points, is calculated from the equation above as $D(A;6)=S(A;6)-V(A)6=3-6/2=0$. Similarly, the local discrepancies of $B$ and $C$ are $0.1267$ and $0.8$, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and save the data file NumInt_Discrepancy.png\n",
    "url = ('https://raw.githubusercontent.com/rickecon/Notebooks/' +\n",
    "       'master/NumIntegr/images/NumInt_Discrepancy.png')\n",
    "image_file = requests.get(url, allow_redirects=True)\n",
    "open('images/NumInt_Discrepancy.png', 'wb').write(image_file.content)\n",
    "Image('images/NumInt_Discrepancy.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A global concept of discrepancy is given by the star-discrepancy. We define the star-discrepancy $\\Delta(N)$ of $N$ points by,\n",
    "\\begin{equation}\\label{EqQMCGlobalDiscrep}\n",
    "  \\Delta(N)=\\sup_J |D(J;N)|,\n",
    "\\end{equation}\n",
    "where the supremum is taken over all subsets $J$ of the form $J=\\prod_{i=1}^s[0,u_i)$. The star-discrepancy can be thought of as the worst-case local discrepancy, looking only at subintervals that have the origin as a corner.\n",
    "\n",
    "A common class of sequences used in quasi-Monte Carlo sampling is equidistributed sequences. Equidistributed sequences sequences where the star-discrepancy $\\Delta(N)$ tends to zero as $N$ tends to infinity. In other words, in the limit the proportion of terms falling in any subinterval is proportional to the length of that interval.\n",
    "\n",
    "There are a number of equidistributed sequences, and here we will provide some examples. By way of notation, let $p_1, p_2, \\ldots$ denote the sequence of prime numbers $2,3,5,\\ldots$, and let $\\langle x\\rangle$ represent the fractional part of $x$, that is $\\langle x\\rangle = x - \\lfloor x\\rfloor$, where $\\lfloor x\\rfloor$ is the rounded down integer of $x$. Thus each element of vector $\\langle x\\rangle$ is between 0 and 1. The table below contains formulas for a number of $s$-dimensional equidistributed sequences on $[0,1)^s$. The figure below shows the first 1,000 points for two-dimensional Weyl, Haber, Niederreiter, and Baker sequences.\n",
    "\n",
    "| Name of Sequence | Formula for $(x_1, x_2, \\ldots, x_s)_n$ |\n",
    "| ---------------- | --------------------------------------- |\n",
    "| Weyl             | $(\\langle np_1^{1/2}\\}, \\ldots, \\langle np_s^{1/2}\\})$ |\n",
    "| Haber            | $\\left(\\left\\langle\\frac{n(n+1)}{2}p_1^{1/2}\\right\\rangle,\\ldots,\\left\\langle\\frac{n(n+1)}{2}p_s^{1/2}\\right\\rangle\\right)$ |\n",
    "| Niederreiter     | $\\left(\\left\\langle n\\left(2^{1/(s+1)}\\right)\\right\\rangle,\\ldots,\\left\\langle n\\left(2^{s/(s+1)}\\right)\\right\\rangle\\right)$ |\n",
    "| Baker            | $(\\langle ne^{r_1}\\rangle,\\ldots,\\langle ne^{r_s}\\rangle)$, $r_j$ rational and distinct |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and save the data file NumInt_ComparisonWHNB1000.png\n",
    "url = ('https://raw.githubusercontent.com/rickecon/Notebooks/' +\n",
    "       'master/NumIntegr/images/NumInt_ComparisonWHNB1000.png')\n",
    "image_file = requests.get(url, allow_redirects=True)\n",
    "open('images/NumInt_ComparisonWHNB1000.png', 'wb').write(image_file.content)\n",
    "Image('images/NumInt_ComparisonWHNB1000.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the equidistributed sequences shown in the table and figure above rely on ascending sequences of prime numbers, I include some functions to produce these sequences of primes. The first function `isPrime(n)` tells you whether a number $n$ is a prime or not. The second function `primes_ascend(N)` returns a vector of length $N$ of the first $N$ primes starting from minimum value of 2 (this is the default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def isPrime(n):\n",
    "    '''\n",
    "    --------------------------------------------------------------------\n",
    "    This function returns a boolean indicating whether an integer n is a\n",
    "    prime number\n",
    "    --------------------------------------------------------------------\n",
    "    INPUTS:\n",
    "    n = scalar, any scalar value\n",
    "\n",
    "    OTHER FUNCTIONS AND FILES CALLED BY THIS FUNCTION: None\n",
    "\n",
    "    OBJECTS CREATED WITHIN FUNCTION:\n",
    "    i = integer in [2, sqrt(n)]\n",
    "\n",
    "    FILES CREATED BY THIS FUNCTION: None\n",
    "\n",
    "    RETURN: boolean\n",
    "    --------------------------------------------------------------------\n",
    "    '''\n",
    "    for i in range(2, int(np.sqrt(n) + 1)):\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def primes_ascend(N, min_val=2):\n",
    "    '''\n",
    "    --------------------------------------------------------------------\n",
    "    This function generates an ordered sequence of N consecutive prime\n",
    "    numbers, the smallest of which is greater than or equal to 1 using\n",
    "    the Sieve of Eratosthenes algorithm.\n",
    "    (https://en.wikipedia.org/wiki/Sieve_of_Eratosthenes)\n",
    "    --------------------------------------------------------------------\n",
    "    INPUTS:\n",
    "    N       = integer, number of elements in sequence of consecutive\n",
    "              prime numbers\n",
    "    min_val = scalar >= 2, the smallest prime number in the consecutive\n",
    "              sequence must be greater-than-or-equal-to this value\n",
    "\n",
    "    OTHER FUNCTIONS AND FILES CALLED BY THIS FUNCTION:\n",
    "        isPrime()\n",
    "\n",
    "    OBJECTS CREATED WITHIN FUNCTION:\n",
    "    primes_vec     = (N,) vector, consecutive prime numbers greater than\n",
    "                     min_val\n",
    "    MinIsEven      = boolean, =True if min_val is even, =False otherwise\n",
    "    MinIsGrtrThn2  = boolean, =True if min_val is\n",
    "                     greater-than-or-equal-to 2, =False otherwise\n",
    "    curr_prime_ind = integer >= 0, running count of prime numbers found\n",
    "\n",
    "    FILES CREATED BY THIS FUNCTION: None\n",
    "\n",
    "    RETURN: primes_vec\n",
    "    --------------------------------------------------------------------\n",
    "    '''\n",
    "    primes_vec = np.zeros(N, dtype=int)\n",
    "    MinIsEven = 1 - min_val % 2\n",
    "    MinIsGrtrThn2 = min_val > 2\n",
    "    curr_prime_ind = 0\n",
    "    if not MinIsGrtrThn2:\n",
    "        i = 2\n",
    "        curr_prime_ind += 1\n",
    "        primes_vec[0] = i\n",
    "    i = min(3, min_val + (MinIsEven * 1))\n",
    "    while curr_prime_ind < N:\n",
    "        if isPrime(i):\n",
    "            curr_prime_ind += 1\n",
    "            primes_vec[curr_prime_ind - 1] = i\n",
    "        i += 2\n",
    "\n",
    "    return primes_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primes_ascend(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key distinction between quasirandom sequences and pseudorandom sequences is that quasirandom sequences do not \"look like\" random numbers. As can be seen in the figures above, they generally display quite obvious patterns. From the outset, quasirandom sequences are chosen so as to have low discrepancy, and are not encumbered by any other requirements of random numbers.\n",
    "\n",
    "However, equidistribution is a rather weak criterion to express the idea that a sequence is uniform. Even though the discrepancy approaches zero as $n$ approaches infinity, the Weyl sequence in the figure above shows how there will often be large gaps for small $n$. An alternative approach is to use low-discrepancy sequences. The goal of a low-discrepancy sequence is to minimize the star-discrepancy of every subsequence. In contrast to equidistributed sequences, the algorithms to generate these sequences take into account the total number of points desired so that maximum uniformity is achieved for every subsequence, and not just in the limit.\n",
    "\n",
    "Halton sequences describe a class of low-discrepancy multidimensional sequences that fill the interval $[0,1)$. To construct a Halton sequence, begin with a consecutive sequence of positive integers of length $N$, for example $n=1,2,\\ldots,N$. Now choose any prime number $p$ and convert each integer $n$ into its representation in the base $p$ number system.  For multiple dimensions, repeat the process with a different prime $p$. Then reflect the base $p$ representation of each integer about the decimal point to obtain a number in the interval $[0,1)$.\n",
    "\n",
    "Many other techniques exist for creating low-discrepancy sequences. Faure' sequences are permutations of Halton sequences. Sobol sequences are a reordering of Halton sequences. Other methods include $(t,m,s)$-Nets and the method of good lattice points. Niederreiter (1978) is a good resource on low-discrepancy sequences in quasi-Monte Carlo methods.\n",
    "\n",
    "Quasi-Monte Carlo methods do far better asymptotically than any Monte Carlo method for many problems. With $N$ points in $s$ dimensions, quasi-Monte Carlo techniques have a worst-case convergence rate of $O \\left( \\frac{\\left(  \\log N\\right) ^s}{N} \\right)$ as opposed to $O \\left( \\frac{1}{\\sqrt{N}} \\right)$ for standard Monte Carlo techniques.  However, standard Monte Carlo integration is easier to implement properly and is generally sufficient for most purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.1.** Use Monte Carlo integration to approximate the value of $\\pi$. Define a function in that takes as arguments a function $g(\\mathbf{x})$ of a vector of variables $\\mathbf{x}$, the domain $\\Omega$ of $\\mathbf{x}$, and the number of random draws $N$ and returns the Monte Carlo approximation of the integral $\\int_\\Omega g(\\mathbf{x}) d\\mathbf{x}$. Let $\\Omega$ be a generalized rectangle--width $x$ and height $y$. In order to approximate $\\pi$, let the functional form of the anonymous function be $g(x,y)$ from Section 4.1 with domain $\\Omega = [-1,1]\\times[-1,1]$. What is the smallest number of random draws $N$ from $\\Omega$ that matches the true value of $\\pi$ to the 4th decimal 3.1415? Set the random seed in your uniform random number generator to 25. This will make the correct answer consistent across submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smallest: 615\n",
      "3.1414634146341465\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as sts\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(seed=25)\n",
    "def inside_or_not(x, y):\n",
    "    if x ** 2 + y ** 2 <= 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "j = 1\n",
    "while j > 0:\n",
    "    x_random = np.random.uniform(-1, 1, j)\n",
    "    y_random = np.random.uniform(-1, 1, j)\n",
    "\n",
    "    values = []\n",
    "    for i in range(j):\n",
    "        value = inside_or_not(x_random[i], y_random[i])\n",
    "        values.append(value)\n",
    "\n",
    "        pii = 4 * np.mean(values)\n",
    "    if round(pii,4) == 3.1415:\n",
    "        print('smallest:',j)\n",
    "        print(pii)\n",
    "        break\n",
    "\n",
    "    else:\n",
    "#         print(j,pii)\n",
    "        j = j +1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.2.** Define a function in that returns the $n$-th element of a $d$-dimensional equidistributed sequence.  It should have support for the four sequences in the Table in Section 4.2. List the 1,073rd element (a pair of values, each between 0 and 1) of the 2-dimensional ($d=2$) equidistributed of each of the four types of sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def isPrime(n):\n",
    "    for i in range(2, int(np.sqrt(n) + 1)):\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "        else:\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def primes_ascend(N, min_val=2):\n",
    "    \n",
    "    primes_vec = np.zeros(N, dtype=int)\n",
    "    MinIsEven = 1 - min_val % 2\n",
    "    MinIsGrtrThn2 = min_val > 2\n",
    "    curr_prime_ind = 0\n",
    "    if not MinIsGrtrThn2:\n",
    "        i = 2\n",
    "        curr_prime_ind += 1\n",
    "        primes_vec[0] = i\n",
    "    i = min(3, min_val + (MinIsEven * 1))\n",
    "    while curr_prime_ind < N:\n",
    "        if isPrime(i):\n",
    "            curr_prime_ind += 1\n",
    "            primes_vec[curr_prime_ind - 1] = i\n",
    "        i += 2\n",
    "\n",
    "    return primes_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  5,  7,  9, 11, 13, 15, 17, 19, 21])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "primes_ascend(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "def equi_distr(n,d,seq_type):\n",
    "    p = primes_ascend(d, min_val = 2)\n",
    "    if seq_type == 'Weyl':\n",
    "        w_list = [math.modf(i)[0] for i in np.sqrt(p)*n]\n",
    "        return w_list\n",
    "    elif seq_type == 'Haber':\n",
    "        h_list = [math.modf(i)[0] for i in np.sqrt(p)*n*(n+1)/2]\n",
    "        return h_list\n",
    "    elif seq_type == 'Niederreiter':\n",
    "        power = [i/(n+1) for i in range(1,d+1)]\n",
    "        n_list = [math.modf(i)[0] for i in (np.power(2,power))*n]\n",
    "        return n_list \n",
    "    elif seq_type == 'Baker':\n",
    "        b_list = [math.modf(i)[0] for i in n*np.exp(p)]\n",
    "        return b_list\n",
    "    else:\n",
    "        return 'No this type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9452804946532538,\n",
       " 0.0657955128830281,\n",
       " 0.16579214229204808,\n",
       " 0.4196378769192961,\n",
       " 0.7085759890615009]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equi_distr(5,5,'Baker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0710678118654755,\n",
       " 0.18033988749894903,\n",
       " 0.2287565553229527,\n",
       " 0.0,\n",
       " 0.5831239517769973]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equi_distr(5,5,'Weyl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.21320343559642652,\n",
       " 0.5410196624968506,\n",
       " 0.6862696659688581,\n",
       " 0.0,\n",
       " 0.7493718553309918]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equi_distr(5,5,'Haber')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6123102415468651,\n",
       " 0.2996052494743662,\n",
       " 0.0710678118654755,\n",
       " 0.9370052598409968,\n",
       " 0.9089871814033934]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equi_distr(5,5,'Niederreiter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.3** Repeat Exercise 4.1 to approximate the value of $\\pi$, this time using quasi-Monte Carlo integration.  You will need to appropriately scale the equidistributed sequences. Compare the rates of convergence. What is the smallest number of random draws $N$ from $\\Omega$ for the quasi-Monte Carlo integration that matches the true value of $\\pi$ to the 4th decimal 3.1415 for each of the four equidistributed sequence types?."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_draw(seq_type):\n",
    "    np.random.seed = 25\n",
    "    N = 1\n",
    "    while N >= 0:\n",
    "        x_draws = [2 * equi_distr(i, 2, seq_type)[0] - 1 for i in range(N)]\n",
    "        y_draws = [2 * equi_distr(i, 2, seq_type)[1] - 1 for i in range(N)]\n",
    "        fraction_x_y = 4* np.mean(list(map(inside_or_not, x_draws, y_draws)))\n",
    "        if round(fraction_x_y, 4) == 3.1415:\n",
    "            print('the smallest number of random draws for %s : %d' % (seq_type, N))\n",
    "            print('circle area is', fraction_x_y)\n",
    "            break\n",
    "        elif N > 5000:\n",
    "            print('%s not coverging to 3.1415' % seq_type)\n",
    "            break\n",
    "        else:\n",
    "            N += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weyl not coverging to 3.1415\n"
     ]
    }
   ],
   "source": [
    "random_draw('Weyl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the smallest number of random draws for Haber : 1901\n",
      "circle area is 3.1415044713308786\n"
     ]
    }
   ],
   "source": [
    "random_draw('Haber')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Niederreiter not coverging to 3.1415\n"
     ]
    }
   ],
   "source": [
    "random_draw('Niederreiter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the smallest number of random draws for Baker : 848\n",
      "circle area is 3.141509433962264\n"
     ]
    }
   ],
   "source": [
    "random_draw('Baker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the convergence, 'Weyl' > 'Baker' > 'Haber' > 'Niederreiter'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sparse Grids\n",
    "Sparse grid interpolation is a method of approximating functions with many dimensions. A direct way of representing multidimensional functions is to use a full grid, such as in Newton-Cotes quadrature. However, using this method the number of grid points that have to be computed depends exponentially on the number of dimensions. Because of this curse of dimensionality, processing functions of beyond five or six dimensions becomes intractable using the techniques of Sections 2 and 3. While Monte Carlo methods perform reasonably well in multidimensional integration, sparse grids remain the most efficient.\n",
    "\n",
    "The sparse grid method selects the nodes of integration by a special truncation of the tensor product expansion of a one-dimensional multilevel selection of nodes. To construct a sparse grid, we first select a series of one-dimensional quadrature formulas indexed by $l$ for a univariate function $f$, and write it as\n",
    "\\begin{equation}\\label{EqSGQuad}\n",
    "  Q_l^{(1)}f = \\sum_{i=1}^{N_l}w_{li}f(x_{li}).\n",
    "\\end{equation}\n",
    "For example, the nodes and weights of this initial quadrature formula series could be based on the trapezoid rule or Simpson's rule with $N_l$ nodes. Now define the difference formulas by,\n",
    "\\begin{equation}\\label{EqSGDiff}\n",
    "  \\Delta_k^{(1)}f = (Q_k^{(1)}-Q_{k-1}^{(1)})f\n",
    "\\end{equation}\n",
    "where $Q_0^{(1)}f=0$.  Note that the differences $\\Delta_k^{(1)}f$ are just univariate quadrature formulas.\n",
    "\n",
    "For a given level $l\\in \\mathbb{N}$, the sparse gride integration approximation for a $d$-dimensional function $f$ is given by\n",
    "\\begin{equation}\\label{EqSGSparseGrid}\n",
    "  Q_l^{(d)} = \\sum_{||\\mathbf{k}||\\leq l+d-1}(\\Delta_{k_1}^{(1)}\\otimes \\ldots \\otimes \\Delta_{k_d}^{(1)})f.\n",
    "\\end{equation}\n",
    "Using the quadrature rule in the equation above, every possible tensor product of the difference formulas is considered, but only those whose sum of indices is smaller than the constant $l+d-1$ are used.\n",
    "\n",
    "The selcetion of nodes in a two-dimensional sparse grid of level $l=3$ using the trapezoid rule as the intial formula is visualized in the figure below. Along the top and left are the one-dimensional grid points for $l=1,2,3$ in the $x$- and $y$-directions. These points are used to create the corresponding product grids $\\Delta_{k_1}\\otimes\\Delta_{k_2}$ for $1\\leq k_1,k_2\\leq3$. Because the grid points used in the trapezoid rule are nested, many of the grid points are cancelled out, and the only ones that remain are formed from a union of the grids along the diagonal as indicated by the black line. The resulting sparse grid $Q_3^{(2)}$ is shown on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'images/NumInt_TrapezoidGrid.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-48d078e0d3af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m        'master/NumIntegr/images/NumInt_TrapezoidGrid.png')\n\u001b[0;32m      4\u001b[0m \u001b[0mimage_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'images/NumInt_TrapezoidGrid.png'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'images/NumInt_TrapezoidGrid.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'images/NumInt_TrapezoidGrid.png'"
     ]
    }
   ],
   "source": [
    "# Download and save the data file NumInt_TrapezoidGrid.png\n",
    "url = ('https://raw.githubusercontent.com/rickecon/Notebooks/' +\n",
    "       'master/NumIntegr/images/NumInt_TrapezoidGrid.png')\n",
    "image_file = requests.get(url, allow_redirects=True)\n",
    "open('images/NumInt_TrapezoidGrid.png', 'wb').write(image_file.content)\n",
    "Image('images/NumInt_TrapezoidGrid.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Discrete Markov Approximation of Continuous AR(1) Process\n",
    "Suppose you have a random shock $z_t$ in your model that has some persistence according to the following AR(1) process.\n",
    "\\begin{equation}\\label{NumInt_EqAR1}\n",
    "  z_{t+1} = \\rho z_{t} + (1-\\rho)\\mu + \\varepsilon_{t+1} \\quad\\text{where}\\quad \\varepsilon_t\\sim N(0,\\sigma) \\quad\\text{and}\\quad \\rho\\in(0,1)\n",
    "\\end{equation}\n",
    "The expected value of $z_{t+1}$ is conditional on the current realization of the shock $E[z_{t+1}|z_t] = \\rho z_{t} + (1-\\rho)\\mu$ but the variance of $z_{t+1}$ is unconditional $Var[z_{t+1}] = \\sigma^2$. The AR(1) process in this equation generates a variable that fluctuates around its mean $\\mu$, and the expected value of the variable tomorrow $E[z_{t+1}|z_t]$ is some convex combination of the variable today $z_t$ and the mean $\\mu$.\n",
    "\n",
    "Typical examples of these types of shocks in economics are shocks to ability, health status, and productivity shocks--all of which exhibit persistence or dependence on recent values. If the shock must be strictly positive, as is the case with productivity shocks, the variable $z_t$ is simply exponentiated.\n",
    "\\begin{equation}\\label{NumInt_EqStochProd}\n",
    "  Y_t = A_t K_t^\\alpha L_t^{1-\\alpha} \\quad\\text{where}\\quad A_t = e^{z_t}\n",
    "\\end{equation}\n",
    "Notice that the variable $A_t$ is lognormally distributed $A_t\\sim LN\\bigl(\\rho z_{t-1} + (1-\\rho)\\mu,\\sigma\\bigr)$ because $\\log(A_t)=z_t$ and $z_t\\sim N\\bigl(\\rho z_{t-1} + (1-\\rho)\\mu,\\sigma\\bigr)$. You made a discretized approximation of the i.i.d. (no persistence) version of this distribution in Exercise 2.3 and estimated average income in the U.S. using it in Exercise 2.4.\n",
    "\n",
    "Tauchen and Hussey (1991) describe a quadrature-based method for producing efficient nodes and probabilities of a discrete first-order Markov process to approximate a continuous AR(1) random variable. Tauchen (1986) details a simpler non-quadrature based method for producing efficient nodes and probabilities of a discrete first-order Markov process to approximate a continuous AR(1) random variable. A classic example of where this discretization is extremely valuable is in the stochastic intertemporal Euler equation from Section 1.\n",
    "\\begin{equation}\\tag{\\ref{NumInt_EqEulEx}}\n",
    "  \\begin{split}\n",
    "    u'(c_t) &^= \\beta E_{z_{t+1}|z_t}\\Bigl[(1+r_{t+1}-\\delta)u'(c_{t+1})\\Bigr] \\\\\n",
    "    \\Rightarrow\\quad u'(c_t) &= \\beta\\int_a^b \\Bigl(1+r_{t+1}(z_{t+1})-\\delta\\Bigr)u'\\Bigl(c_{t+1}(z_{t+1})\\Bigr)f(z_{t+1}|z_t)dz_{t+1}\n",
    "  \\end{split}\n",
    "\\end{equation}\n",
    "The expectation on the right-hand-side of the Euler equation is over $z_{t+1}$ given $z_t$, where $z_{t+1}$ is the AR(1) process described at the beginning of this section. One of the most common nonlinear solution techniques for the functional equations of the dynamic household decision problem characterized by this Euler equation is value function iteration on the following recursive Bellman equation.\n",
    "\\begin{equation}\\label{NumInt_EqBellman}\n",
    "  V(k,z) = \\max_{k'}\\:u(k,z,k') + \\beta E_{z'|z}\\bigl[V(k',z')\\bigr]\n",
    "\\end{equation}\n",
    "\n",
    "The expectation on the right-hand-side of the Bellman equation is simply an integral of the form $E_{z'|z}\\bigl[V(k',z')\\bigr] = \\int_{z'}V(k',z')f(z'|z)dz'$. However, it is difficult to use standard Gaussian quadrature or Monte Carlo integration methods because the value function $V(k',z')$ is often only known at a few points.\n",
    "\n",
    "One solution to this problem is to interpolate or fit some continuous function $\\tilde{V}(k',z')$ to the known points of $V(k',z')$ and then use Gaussian quadrature or Monte Carlo integration to approximate the integral $\\int_{z'}\\tilde{V}(k',z')f(z'|z)dz'$. Heer and Maussner (2008) and Heer and Maussner (2009, p. 237) find that the errors in the extrapolated values of the interpolated function $\\tilde{V}$ beyond the bounds of the known points of $V$ cause the solution to be less accurate than using the Tauchen-Hussey method of approximating $f(z'|z)$ with a discrete first order Markov process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. References\n",
    "* Adda, Jerome and Russell Cooper, *Dynamic Economics: Quantitative Methods and Applications*, MIT Press (2003).\n",
    "* Armington, Paul S., \"A Theory of Demand for Products Distinguished by Place of Production,\" IMF Staff Papers, 16:1, March (1969).\n",
    "* Dixit, Avinash K. and Joseph E. Stiglitz, \"Monopolistic Competition and Optimum Product Diversity,\" *American Economic Review*, 67:3, pp. 297-308, June (1977).\n",
    "* Heer, Burkhard and Alfred Maussner, \"Computation of Business Cycle Models: A Comparison of Numerical Methods,\" *Macroeconomic Dynamics*, 12:5, pp.641-663, November (2008).\n",
    "* Heer, Burkhard and Alfred Maussner, *Dynamic General Equilibrium Modeling: Computational Methods and Applications*, 2nd edition, Springer (2009).\n",
    "* Judd, Kenneth L., *Numerical Methods in Economics*, MIT Press, (1998).\n",
    "* Niederreiter, Harald, \"Quasi-Monte Carlo Methods and Pseudo-Random Numbers,\" *Bulletin of the American Mathematical Society*, 84:6, November (1978).\n",
    "* Tauchen, George, \"Finite State Markov-chain Approximation to Univariate and Vector Autoregression,\" *Economics Letters*, 20:2, pp. 177-181 (1986).\n",
    "* Tauchen, George and Robert Hussey, \"Quadrature-based Methods for Obtaining Approximate Solutions to Nonlinear Asset Pricing Models,\" *Econometrica*, 59:2, pp. 371-396, March (1991)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
